{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction from Text**\n",
        "\n",
        "Machine learning algorithms require numerical data to function, so raw text must be converted into numerical vectors through a process called feature extraction. This process transforms unstructured text into a structured format that algorithms can interpret. Here are some common techniques for feature extraction:\n",
        "\n",
        "#### 1. **Bag of Words (BoW)**\n",
        "The Bag of Words model represents text data as a matrix of token counts. Each document is converted into a vector where the value for each dimension corresponds to the frequency of a specific word in the document. This method disregards word order but captures word occurrence.\n",
        "\n",
        "#### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
        "TF-IDF is an extension of the Bag of Words model that weighs the word counts by their importance. It combines two metrics:\n",
        "- **Term Frequency (TF):** Measures how frequently a word occurs in a document.\n",
        "- **Inverse Document Frequency (IDF):** Measures how important a word is by considering its frequency across all documents.\n",
        "The resulting vectors reflect both the frequency and importance of words, making TF-IDF useful for identifying significant terms.\n",
        "\n",
        "#### 3. **Word Embeddings**\n",
        "Word embeddings are dense vector representations of words where similar words have similar vectors. These methods capture semantic relationships and contextual meanings, providing a richer representation than simple counts.\n",
        "\n",
        "   - **Word2Vec:** A popular word embedding technique that uses neural networks to learn word associations from large text corpora. It creates vectors where words with similar meanings are close in the vector space.\n",
        "\n",
        "   - **GloVe (Global Vectors for Word Representation):** Another word embedding method that factors in the global word-word co-occurrence matrix from a corpus. GloVe generates vectors based on the frequency of word co-occurrences across the entire dataset.\n",
        "\n",
        "These feature extraction techniques help transform textual data into a format suitable for machine learning models, allowing algorithms to perform tasks such as classification, clustering, and sentiment analysis more effectively.\n",
        "\n"
      ],
      "metadata": {
        "id": "M4VXncUDuyUm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Bag of Words (BoW)**\n",
        "\n",
        "**Bag of Words (BoW)** is a widely used technique for converting text into numerical features, making it compatible with machine learning algorithms. It offers a straightforward and versatile approach to text representation, which is helpful for various natural language processing (NLP) tasks. Below is a detailed overview:\n",
        "\n",
        "#### **What is Bag of Words?**\n",
        "\n",
        "BoW is a method that transforms text data into numerical vectors by capturing the frequency of words within a document. The key idea is to represent the text based on the presence or absence of words, without considering the order in which they appear.\n",
        "\n",
        "**Key Components:**\n",
        "1. **Vocabulary:** A collection of all unique words found in the text corpus (the entire set of documents).\n",
        "2. **Feature Matrix:** A table where each row corresponds to a document and each column represents a word from the vocabulary. The values in the matrix denote the frequency of each word in the corresponding document.\n",
        "\n",
        "#### **How Does It Work?**\n",
        "\n",
        "1. **Creating the Vocabulary:**\n",
        "   - Identify all distinct words in the corpus.\n",
        "   - For instance, given the following reviews:\n",
        "     - \"Good hotel\"\n",
        "     - \"Not a good hotel\"\n",
        "     - \"Best hotel in the area\"\n",
        "   - The vocabulary would include: `good`, `hotel`, `not`, `a`, `best`, `in`, `the`, `area`.\n",
        "\n",
        "2. **Building the Feature Matrix:**\n",
        "   - Each row in the matrix represents a document, and each column corresponds to a word in the vocabulary.\n",
        "   - The matrix is filled with word counts or binary values indicating the presence of words.\n",
        "\n",
        "   Here’s an example matrix for the provided reviews:\n",
        "\n",
        "\n",
        "|              | good | hotel | not | a | best | in | the | area |\n",
        "|--------------|------|-------|-----|---|------|----|-----|------|\n",
        "| **First Review**  | 1    | 1     | 0   | 0 | 0    | 0  | 0   | 0    |\n",
        "| **Second Review** | 1    | 1     | 1   | 1 | 0    | 0  | 0   | 0    |\n",
        "| **Third Review**  | 0    | 1     | 0   | 0 | 1    | 1  | 1   | 1    |\n",
        "\n",
        "\n",
        "#### **Drawbacks of Bag of Words:**\n",
        "\n",
        "1. **Dominance of Frequent Words:**\n",
        "   - Common words may overshadow less frequent but potentially important words, affecting the feature representation.\n",
        "\n",
        "2. **Loss of Word Order:**\n",
        "   - BoW disregards the sequence of words, which means information about the syntax and semantics related to word order is lost.\n",
        "\n",
        "3. **Large and Sparse Matrices:**\n",
        "   - With a vast vocabulary, the feature matrix can become very large and mostly empty (sparse), leading to high memory usage and computational inefficiency.\n",
        "\n",
        "Despite these limitations, Bag of Words remains a fundamental technique in text processing due to its simplicity and effectiveness for many text classification tasks."
      ],
      "metadata": {
        "id": "ovfJ1d-wxFE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    'I love programming in Python.',\n",
        "    'Python is great for data analysis.',\n",
        "    'Machine learning with Python is fun.'\n",
        "]\n",
        "\n",
        "# Initialize the CountVectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "bag_of_words = count_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame with the bag-of-words matrix\n",
        "df_bow = pd.DataFrame(bag_of_words.toarray(), columns=feature_names)\n",
        "\n",
        "print(df_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKkiv8kh739k",
        "outputId": "a0ca22aa-8a67-41b2-e5f5-14102f754a6b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   analysis  data  for  fun  great  in  is  learning  love  machine  \\\n",
            "0         0     0    0    0      0   1   0         0     1        0   \n",
            "1         1     1    1    0      1   0   1         0     0        0   \n",
            "2         0     0    0    1      0   0   1         1     0        1   \n",
            "\n",
            "   programming  python  with  \n",
            "0            1       1     0  \n",
            "1            0       1     0  \n",
            "2            0       1     1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Term Frequency–Inverse Document Frequency (TF-IDF)**\n",
        "\n",
        "**Term Frequency–Inverse Document Frequency (TF-IDF)** is a statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). TF-IDF helps to identify words that are significant within a document but not necessarily common across the entire corpus. This method addresses some limitations of the Bag of Words (BoW) model by emphasizing words that are rare and more informative.\n",
        "\n",
        "#### **How TF-IDF Works**\n",
        "\n",
        "TF-IDF is composed of two main parts:\n",
        "\n",
        "1. **Term Frequency (TF):**\n",
        "   - **Term Frequency** measures how often a term appears in a document. It can be seen as the probability of finding a word within that document.\n",
        "   - Formula:\n",
        "     ```\n",
        "     TF = (Number of times term t appears in a document) / (Total number of terms in the document)\n",
        "     ```\n",
        "   - For example, if a document contains 100 words and the word \"cat\" appears 3 times, the TF for \"cat\" would be:\n",
        "     ```\n",
        "     TF = 3 / 100 = 0.03\n",
        "     ```\n",
        "\n",
        "2. **Inverse Document Frequency (IDF):**\n",
        "   - **Inverse Document Frequency** measures the importance of a term across the entire corpus. It decreases if the term appears in many documents, and increases if it appears in fewer documents. This helps to down-weight common words and up-weight rare words.\n",
        "   - Formula:\n",
        "     ```\n",
        "     IDF = log((Total number of documents D) / (Number of documents containing term t))\n",
        "     ```\n",
        "   - For instance, if there are 10 million documents in the corpus and the word \"cat\" appears in 1,000 of them, the IDF would be:\n",
        "     ```\n",
        "     IDF = log(10,000,000 / 1,000) = 4\n",
        "     ```\n",
        "\n",
        "3. **TF-IDF Calculation:**\n",
        "   - The TF-IDF weight is the product of TF and IDF, representing the term's importance in the document relative to the entire corpus.\n",
        "   - Formula:\n",
        "     ```\n",
        "     TF-IDF = TF * IDF\n",
        "     ```\n",
        "   - For the word \"cat\" in the previous example, the TF-IDF weight would be:\n",
        "     ```\n",
        "     TF-IDF = 0.03 * 4 = 0.12\n",
        "     ```\n",
        "\n",
        "#### **Advantages Over Bag of Words (BoW)**\n",
        "\n",
        "- **Addressing Dominance of Frequent Words:**\n",
        "  - Unlike BoW, which treats all words equally, TF-IDF adjusts the weight of each word by considering its frequency in the document and its rarity across the corpus. This reduces the dominance of frequently occurring words and highlights words that are significant but less common.\n",
        "\n",
        "#### **Example Application**\n",
        "\n",
        "Using TF-IDF, each word's weight in the hotel reviews would be calculated to better capture the importance of words. For instance, terms that are more specific to a document and less common across the corpus would receive higher TF-IDF scores.\n",
        "\n"
      ],
      "metadata": {
        "id": "XUB-f6_j7tNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    'I love programming in Python.',\n",
        "    'Python is great for data analysis.',\n",
        "    'Machine learning with Python is fun.'\n",
        "]\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the corpus\n",
        "values = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Get feature names\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Create a DataFrame with the TF-IDF matrix\n",
        "df_tfidf = pd.DataFrame(values.toarray(), columns=feature_names)\n",
        "\n",
        "print(df_tfidf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyeRO6T7Pv-6",
        "outputId": "43578df0-eb15-4840-8d7b-b709d402aac2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   analysis      data       for       fun     great        in       is  \\\n",
            "0  0.000000  0.000000  0.000000  0.000000  0.000000  0.546454  0.00000   \n",
            "1  0.450504  0.450504  0.450504  0.000000  0.450504  0.000000  0.34262   \n",
            "2  0.000000  0.000000  0.000000  0.450504  0.000000  0.000000  0.34262   \n",
            "\n",
            "   learning      love   machine  programming    python      with  \n",
            "0  0.000000  0.546454  0.000000     0.546454  0.322745  0.000000  \n",
            "1  0.000000  0.000000  0.000000     0.000000  0.266075  0.000000  \n",
            "2  0.450504  0.000000  0.450504     0.000000  0.266075  0.450504  \n"
          ]
        }
      ]
    }
  ]
}